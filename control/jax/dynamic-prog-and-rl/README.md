# Exact Dynamic Programming & Approximate Policy Iteration (DP & API)
Here with DP we refer to the classic solution for the Bellman equation in a grid space, and we used the Soft-Actor-Critic (SAC) as the API algorithm, Haarnoja (2018). The solution with SAC is very good, we recommend to be careful with the scale of the environment i.e., a reward function must have some dimension consciousness. Very big rewards were obtained when using kJ as a measure for the reward function which later didn't became smaller (the Policy didn't converge over time), but using kWh the policy converged to a locally optimal value.

Also note that in this folder the system is written in JAX, this because CasADi symbolic expressions are difficult to work well when combined with `jax.Array` data. The 'stochastic-mpc' folder contains a Stochastic Model Predictive Control (SMPC) Model.